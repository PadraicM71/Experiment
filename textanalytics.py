# -*- coding: utf-8 -*-

'''
pip install sklearn
pip install nltk
pip install wikipedia
'''

"""CCT_TextAnalytics_Lecture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LWYO-DEJ6jjnQKzIL5w7PKQxnEURZC4R

############################## Tokenisation
"""

from sklearn.feature_extraction.text import CountVectorizer

tokenize = CountVectorizer().build_tokenizer()

myText = myText='I am having a TextMining lecture with Dr. John Doe and I am <3 it so much!'
tokenize(myText)

"""############################## Bag Of Words"""

from sklearn.feature_extraction.text import CountVectorizer

D1 = "I am having a Text Mining lecture."
D2 = "I like Text Mining. Text Mining is life."
D3 = "I like text messaging."

vectorizer = CountVectorizer()
bag = vectorizer.fit_transform([D1,D2,D3])  

print(bag)

"""#################### Showing terms with corresponding index"""

vocabulary = vectorizer.vocabulary_
print(vocabulary)

"""############################## N-GRAMS"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(1, 2))

ngrams = vectorizer.fit_transform([D1, D2, D3])
print(ngrams)

vocabulary = vectorizer.vocabulary_
print(vocabulary)

"""#############################. Similarity Measure"""

from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(bag)

"""############## Example showing challenge (with Jaguar)

"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

D1 = "A Jaguar is a costly car"
D2 = "Jaguars are expensive cars"
D3 = "The jaguar, a feline animal"

vectorizer = CountVectorizer()
bag = vectorizer.fit_transform([D1,D2,D3]) 
cosine_similarity(bag)

"""#################### Stop Words (Method 1)"""

from sklearn.feature_extraction.text import CountVectorizer

D1 = "I am having a Text Mining lecture."
D2 = "I like Text Mining. Text Mining is life."
D3 = "I like text messaging."

vectorizer = CountVectorizer(stop_words="english")
bag = vectorizer.fit_transform([D1, D2, D3])

print(bag)

vocabulary = vectorizer.vocabulary_
print(vocabulary)

"""#################### Stop Words (Method 2)"""

myWords = ['having','is', 'am', 'text']

vectorizer = CountVectorizer(stop_words=myWords)

bag = vectorizer.fit_transform([D1, D2, D3])
print(bag)
vocabulary = vectorizer.vocabulary_
print(vocabulary)

"""######################## Stemming"""

#pip install nltk

#import the nltk package
import nltk
#call the nltk downloader
nltk.download('wordnet') # you can also download others such as "punkt"
nltk.download('punkt')

"""############################# Stemming Single Words"""

from nltk.stem.porter import PorterStemmer

words=['cars','computing','having','students','flying','lies']

stemmer = PorterStemmer()
for word in words:
  print(stemmer.stem(word))

"""################################# Stemming Words in a Document"""

from nltk.stem.porter import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize

def stemDocument(d):
  token_words=word_tokenize(d)
  token_words
  stem_document=[]
  stemmer = PorterStemmer()
  for word in token_words:
    stem_document.append(stemmer.stem(word))
  return " ".join(stem_document)

D1 = "The glory of computers does not lie in never falling, but in computing rising every time it falls"
stemD1 = stemDocument(D1)

print(stemD1)

"""################################# Lemming Single Words"""

from nltk.stem import WordNetLemmatizer

words=['cars','computing','having','students','flying','lies']

lemmatizer = WordNetLemmatizer()
for word in words:
  print(lemmatizer.lemmatize(word))

"""################################# Lemmatisation Words in a Document"""

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize, word_tokenize

def lemDocument(d):
  token_words=word_tokenize(d)
  token_words
  lem_document=[]
  lemmatizer = WordNetLemmatizer()
  for word in token_words:
    lem_document.append(lemmatizer.lemmatize(word))
  return " ".join(lem_document)

D1 = "The glory of computers does not lie in never falling, but in computing rising every time it falls"
lemD1 = lemDocument(D1)

print(lemD1)

"""###################################### Low Frequency Filtering"""

from sklearn.feature_extraction.text import CountVectorizer

D1 = "I am having a Text Mining lecture."
D2 = "I like Text Mining. Text Mining is life."
D3 = "I like text messaging."

vectorizer = CountVectorizer(min_df=2)

bag = vectorizer.fit_transform([D1, D2, D3])
print(bag)
vocabulary = vectorizer.vocabulary_
print(vocabulary)

"""###################################### Term Weighting"""

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

D1 = "I am having a Text Mining lecture."
D2 = "I like Text Mining. Text Mining is life."
D3 = "I like text messaging."

bag = vectorizer.fit_transform([D1, D2, D3])
print(bag)
vocabulary = vectorizer.vocabulary_
print(vocabulary)

"""######################. Exercice (Searching Wikipedia)

'''
You would like to create an application where users submit search queries and you return the most suitable Wikipedia webpage to repond to the query. 

For example: the user submits the query: "Is it true that the father of Barack Obama is borm in Kenya ?" 
You need to search for the webpage in wikipedia that talks about Obama which is the most similar to the query.

'''
"""

#pip install wikipedia

import wikipedia
pageIds=wikipedia.search("Obama")

print("pageIds", pageIds)

pages     = []
for pageId in pageIds:
	try:
		page=wikipedia.page(pageId)
		pages.append(page)
	except:
		print("Oups: Could not parse the page: ", pageId)
		pass

titles    = [page.title for page in pages]	
urls      = [page.url for page in pages]
documents = [page.content for page in pages]

query="In which country is the father of Barack Obama born ?"

documents.append(query) # add query as a document at the end of the documents list

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()


#lowercase each document
lowercaseDocuments = [document.lower() for document in documents]

#lemmatise
lemDocuments = [lemDocument(document) for document in lowercaseDocuments]

# remove stop words and filter words in less than 2 documents, and using TF/IDF
vectorizer = TfidfVectorizer(stop_words="english", min_df=2)

# bag of words using TF/IDF
bag = vectorizer.fit_transform(lemDocuments)

from sklearn.metrics.pairwise import cosine_similarity


cosine_similarity(bag)

import numpy as np
from sympy import Matrix, init_printing
init_printing()
display(Matrix(cosine_similarity(bag)))

print("title=",titles[2])
print("url=",urls[2])
print("document=",documents[2])

"""##### Try with N-Grams"""

# Same as before but with ngram size 2
vectorizer = TfidfVectorizer(stop_words="english", min_df=2, ngram_range=(2, 2))

# bag of words using TF/IDF
bag = vectorizer.fit_transform(lemDocuments)


vocabulary = vectorizer.vocabulary_

import numpy as np
from sympy import Matrix, init_printing
init_printing()
display(Matrix(cosine_similarity(bag)))

print("title=",titles[2])
print("url=",urls[2])
print("document=",documents[2])